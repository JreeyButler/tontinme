------inode
------iops
------raid介绍
------日志文件系统

#inode
	inode是UNIX操作系统中的一种数据结构，它包含了与文件系统中各个文件相关的一些重要信息。通常，文件磁盘空间中大约百分之一
		空间分配给了inode表
	每个inode节点的大小，一般是128字节或256字节，inode节点的总数，在格式化时就给定，一般是每1KB或2KB就设置一个inode
		$dumpe2fs -h /dev/sda | grep "Inode size"	#查看每个inode节点的大小
		$df -i	#查看每个硬盘分区的inode总数和已经使用的数量
	当用户搜索或者访问一个文件时，UNIX系统通过inode表查找到正确的inode编号(inumber)，在找到inode编号之后，相关的命令才可以
		访问该inode，并对其进行适当的更改。如果同时有另一个用户试图对同一个文件进行编辑，由于inode已经被被分配给了第一
		个用户，则该用户只能等待
		$ls -i 可以查看文件的索引编号
	注：inode和索引编号(inumber)这两个术语非常相似，并且相互关联，但它们所指的并不是同样的概念，inode指的是数据结构，而
		索引编号实际上是inode的标识编号
	linux中使用stat查看文件的具体inode信息
		$stat XXX中的Inode即是inode索引编号，和ls -i XXX的效果是一样的
	inode的结构
		*inode编号
		*用来识别文件类型，以及用于stat C函数的模式信息
		*文件的链接数目
		*属主的UID
		*属主的GID
		*文件的大小
		*文件所使用的磁盘块的实际数目
		*mtime(modify time)	#文件内容上一次变动的时间
		*atime(access time)	#文件上一次打开的时间
		*ctime(change time)	#inode上一次变动的时间
		从根本上讲，inode中包含有关文件的所有信息（除了文件的实际名称以及实际数据内容之外）
	由于某种原因，某个文件系统的inode使用率达到100%后，您将无法在该文件系统中创建更多的文件、设备或目录。
		对于这种情况，一种解决办法是通过smitty chfs命令(AIX)为该文件系统添加更多的空间;另一种是创建较小的inode区段
	fsck可以用来修复文件系统或者修正受损的inode
	硬链接
		文件名不同，但是指向相同的inode。可以用不同的文件名访问相同的内容，对文件内容进行修改，会影响到所有文件名，但是
			删除一个文件名，不影响另一个文件名的访问，这种情况就称为'inode'
	软链接/符号链接
		inode相同，但是文件A的内容是文件B的路径，用户读取文件A时，系统会自动将访问者导向文件B
	Note:关于硬/软链接的具体介绍可以查看linux/linux_system的相关笔记
	inode的特殊作用
		1. 有时，文件名包含特殊字符，无法正常删除。这时，直接删除inode节点，就能起到删除文件的作用
			$find . -inum [inode num] -exec rm -i {} \;
		2. 移动文件或重命名文件，只是改变文件名，不影响inode索引编号
		3. 打开一个文件后，系统就以inode索引编号来识别这个文件，不再考虑文件名
		第三点使得软件更新变的简单，可以在不关闭软件的情况下进行更新，不需要重启。因为系统通过inode号码，识别运行中的文件，
			更新的时候，新版文件以同样的文件名，生成一个新的inode，不会影响到运行中的文件，等到下一次运行这个软件的
			时候，文件名就自动指向新版文件，旧版文件的inode则被回收
#iops
	input/output operation per second
	IOPS is a common performance measurement used to benchmark computer storage devices like hard disk drives(HDD)
		solid state drivers(SSD),and storage area networks(SAN). As with any benchmark, IOPS numbers published
		by storage device manufactures do not guarantee real-world application performance
	即每秒进行读写操作(I/O)的次数，多用于数据库等场合，衡量随机访问的性能
	存储端的iops性能和主机端的IO是不同的，IOPS是指存储每秒可接受多少次主机发出的访问，主机的一次IO需要多次访问
		存储才可以完成
	The most common performance characteristic are as follows:
		measurement		Description
		Total IOPS		Total number of I/O operation per second (when performing a mix of read and write tests)
		Random Read IOPS	Average number of random read I/O operations per second
		Random Write IOPS	Average number of random write I/O operations per second
		Random Read IOPS	Average number of sequential read I/O operations per second
		Random Write IOPS	Average number of sequential write I/O operations per second
	For HDDs and similar electromechanical storage devices, the random IOPS numbers are primarily dependent upon the 
		storage device's random seek time(寻道时间) 	'
	For SSDs and similar state storage devices, the random IOPS numbers are primarily dependent upon the storage device's   '
		internal controller and memory interface speeds.
	On both types of storage devices the sequential IOPS numbers(especial when using a large block size) typically indicate 
		the maximum sustained bandwidth that the storage device can handle
	Often sequential IOPS are reported as a simple MB/s number as follows:
		IOPS*TransferSizeInBytes = BytesPerSec(with the answer typically converted to MegabytesPerSec)
	EXAMPLE:
		Device			Type	IOPS		Interface
		7,200 rpm SATA drives	HDD	~75-100 IOPS	SATA 3 Gb/s
		10,000 rpm SATA drives	HDD	~125-150 IOPS	SATA 3 Gb/s
		15,000 rpm SAS drives	HDD	~175-210 IOPS	SAS

		#Solid State Devices
		http://en.wikipedia.org/wiki/IOPS
#raid介绍
	RAID(Redundant Array of Independent Disks), one of the most common techniques to improve either data reliability
		or data performance (or both).
	The core RAID configurations are listed as:RAID-0,RAID-1,RAID-2,RAID-3,RAID-4,RAID-5, and RAID-6
	
	RAID-0
	1.This RAID configuration is really focused on performance since the blocks are basically striped across multiple disks.
	2.How the data is written to two disks
		the first block of data A0, is written to the first disk,the second block of data A1, is written to the second disk,
			the third block of data A3, is written to the firsh disk, and so on.
		If the I/O is happening fash enough data blocks can be written almost simultaneously(A0 and A1 are written at just 
			about the same time)
	3.Reading from a RAID-0 group is almostso very fast. A read request comes in and the RAID controller, which controls the placement
		of data, knows that it can read A0 and A1 at the same time since they are on separate disks, basically doubling the 
		potential read performance relative to a single disk.
	4.You can have as many disks as you want in a RAID-0 array (a group of disks in a RAID-0 configuration). However, if you lose 
		a disk in a RAID-0 array, you will lose access to all of the data in the array. If you can bring the lost disk back
		into the array without losing any data on it. then you can recover the RAID-0 array, but this is a fairly rare occurrence.
	5.RAID-0 is focused solely on performance with no additional data redundancy beyond the redundancy in a single disk.This affects 
		how RAID-0 is used. For example, it can be used in situations where performance is paramount and you have a copy of 
		your data elsewhere or the data is not import.
	6.The capacity is computed as:
		Capacity = n * min(disk sizes)
		#n is the number of disks in the Array
		#min(disk sizes) is the minimum common capacity across the drivers (this indicates that you can use drives of different 
			size)
	7.The failure rate is a little more involved but can also be estimated
		MTTFgroup = MTTFdisk / n
		#n is number of disks in the array
		#MTTF is the Mean Time To Failure(平均故障时间) and "group" refers to the RAID-0 array and "Disk" refers to a single disk.
	8.RAID-0 is the fastest RAID configuration and the best capacity utilization of any RAID configuration discussed in this article
	 Table about RAID-0
	 Raid Level	Pros					Cons		Storage Efficiency			Minimum Number of disks
	 RAID-0		*Performance(great read and write 	*No data 	100% assuming the drives are 		2
	 		performance)				redundancy	the same size	
			*great capacity(the best of any		*Poor MTTF
			standard RAID cofigurations)
	RAID-1
	1.How the data is written to RAID-1
		when block A1 is written to disk 0, the same block is also written to disk 1. Since the disks are independent of one another
			the write to disk 0 and the write to disk 1 can happen at the same time.
	2.when the data is read, the RAID controller can read block A1 from disk 0 and block A2 from disk 1 at the same time since the disk
		are independent. So overall, the write performance of a RAID-1 array s the same as a single disk and the read performance 
		acturally faster from a RAID-1 array relative to a single disk
	3.The capacity of RAID-1 is the following:
		capacity = min(disk sizes)
		#meaning that the capacity of RAID-1 is limited by the smallest disk(you can use different size drives in RAID-1). 
		RAID-1 has the lowest capacity utilization of any RAID configuration
	4.The reliability or probability of failure of RAID-1 is the following:
		P(dual failure) = P(single drive) * P(single drive)	#square of the failure of a single drive
		#Since the probability of failure of a single drive is Levelss than 1, that means that the failure of a RAID-1 array is even
			#smaller than the probability of failure of a single drive

	RAID-2
	1.This RAID level was one of the original five defined, but it is no longer really used.
	2.The basic concept is that RAID-2 stripes data at the bit level instead of the block level and uses a Hamming coding for parity computations
	3.In RAID-2, the first bit is written on the first drive, the second bit is written on the second drive, and so on. Then a Hamming-code
		parity is computed and either stored on the disks or on a separate disk.	
	4.Since hard drives added error correction that used Hamming-code, so using them at the RAID level became redundant so people stopped
		using RAID-2

	RAID-3
	1.RAID-3 uses data striping at the byte level and also adds parity computations and sotres them on a delicated parity disk
	2.The RAID-3 layout uses 4 disks and stripes data across three of them and uses the fourth disk for storing parity information
		so a chunk of data "A" has byte A1 written to disk 0, byte A2 is written to disk 1, and bytes A3 written to disk 2.
		then the parity of bytes A1 A2 and A3 is computed and written to disk 3. The process then repeats until the entire chunk 
		of data "A" is written.
	3.The minimum number of disks you can have in RAID-3 is three (you need 2 data disks and a third disk to store the parity)
	4.In particular, RAID-3 cannot accommodate multiple requests at the same time. This result from the fact that a block of data will be
		spread across all members of the RAID-3 group (minux the parity disk) and the data has to reside in the same location on each drive
	5.RAID-3 is not very popular in the real-world but from time to time you do see it used. RAID-3 is used in situations where 
		RAID-0 is totally unacceptale because there is not enough data redundancy and the data throughout reduction due to the data 
		parity computations is acceptable

	RAID-4
	1.RAID-4 builds on RAID-0 by adding a parity disk to block-level striping. Since the striping is now down to a block level each disk
		can be accessed independently to read or write data allowing multiple data access to happen at the same time.
	2.In this layout,data is written in block stripes to the first three disks (disk0, disk1, disk2), while disk 3 it the parity drive.
		the parity of the blocks accross the drives is computed by the RAID controller and stored on the delicated parity drive
	3.The delicated parity drive becomes a performance bottleneck in RAID-4,particular for write I/O. you can write to blocks A1 and B1 
		at the same time since they are on different disks. However the parity for both blocks has to be written to the same drive 
	       	which can only accommodate a single write I/O request at a tim.
	4.The capacity of RAID-4 is the following:
		Capacity = min(disk sizes) * (n-1)
	5.RAID-4 is rarely used because RAID-5 has replaced it 

	RAID-5
	1.RAID-5 is similar to RAID-4 but now the parity is distributed across all of the drives instead of using a dedicated parity drive
		this greatly improves write performance relative to RAID-4 
	2.In this layout, the parity blocks are labeled with a subscript "p" to indicate parity. the blocks that line up(one block per drive)
		are typically a "stripe". The data stripe size is simply the following:
		Data stripe size = block size * (n-1) #n is the number of dirves in the RAID-5 array
	3.Anytime a block inside the stripe is changed or written to, the parity block is recomputed and rewritten (this is sometimes called 
		the read-modify-write process). 
	4.RAID-5 has some write performance for small writes that are smaller than a single stripe since the parity needs to be computed several
		times which eats up computational capability of the RAID controller.
	5.The capacity of RAID-5 is very similar to RAID-4 and is the following:
		Capacity = min(disk sizes) * (n-1)
	6.RAID-5 you can lose a single drive and not lose data
	7.There is a new phenomenon that impacts RAID-5 that has been explained in various article around the web. Basically the capacity 
		of drives is growing quicker than the Unrecoverable Read Error(URE) rate of drives to the point where losing a drive in a 
		RAID-5 array and recovering it to a hot-spare drive is almost guaranteed to lead to a URE which means that the RAID-5 array
		will be lost
	8.There is no shortage of article about RAID-5 on the web. You will see some strong opinions both for and against RAID-5 based on
		usage cases.

	RAID-6
	1.As mentioned previously, there is a potential problem with RAID-5 for larger capacity drives and a larger number of them. 
		RAID-6 attempts to help that situation by using two parity blocks per stripe instead of RAID-5's single parity block.		'
		This allows you to lose two drives with losing any data

	2.The first parity block is noted with as sbuscript "p" such as Ap, the second parity block in a stripe is noted with a subscript 
		"q" such as Aq. The use of two parity blocks reduces the useable capacity of a RAID-6 as in the following:
		Capacity = min(disk sizes) * (n-2)
	3.Computing the first parity block,p, is done in the same fashion as RAID-5. However, computing the q parity block is more complicated
		as explained here(http://en.wikipedia.org/wiki/Standard_RAID_levels#Computing_Parity). This means that the write performance
		of a RAID-6 array can be slower than a RAID-5 array for a given level of RAID controller performance
	4.However, read performance from a RAID-6 is just as fast as a RAID-5 array since reading the parity blocks is skipped.

	Hybrid RAID Levels(Nested RAID Levels)
	1.For example, a common configuration is called RAID 1+0 or RAID-10. The first number ( the furthest to the left) refers to the 
		"bottom" or initial part of the RAID array. Then the second number from the left refers to the "top" level or the RAID array.
		the top level RAID uses the bottom level RAID configurations as building blocks.
	2.In the case of RAID-10, the approach is to use multiple pairs of drives at the lowest level (RAID-1) and then to combin them 
		using RAID-0. 
	RAID-01
	1.the capacity of RAID-01 array is computed as:
		Capacity = (n/2) * min (disk sizes)
		#n is the number of disks in the array
	RAID-10
	1.the capacity of RAID-10 array is computed as:
		Capacity = (n/2) * min (disk sizes)
		#n is the number of disks in the array
	RAID-05
	1.the capacity of RAID-05 array is computed as:
		Capacity = min(disk size) * (Number of drivers in each RAID-0 group) * (Number of RAID-0 groups - 1)
	2.the fault tolerance of RAID-05 is limited to really one drive. If you lose one or more drives in a RAID-0 group, then you
		lose that RAID-0 group. You could lose another drive in the same RAID-0 group that has already lost a drive, but
		that requires that you control which drives fail in what order.
	3.If you lose a drive in RAID-05 configuration, then every drive in the configuration is involved in the rebuilding process
	RAID-50
	1.the capacity of RAID-50 array is computed as:
		Capacity = min(disk size) * (Number of drivers in each RAID-5 group - 1) * (Number of RAID-5 groups)
	2.the big different between RAID-05 and RAID-50 is the number of dirves involved in a rebuild.
	In RAID-50 only the surviving drives in the specific RAID-5 group that has a failed drive have to be read and only the single
		replacement drive in the RAID-5 group is written to.
	RAID-15
		...
	RAID-15
		...


	 --------------------------------------------------------------------------------------------------------------------------------------
	 --------------------------------------------------------------------------------------------------------------------------------------
	 Raid Level	Pros					Cons				Storage Efficiency		Minimum Number of disks
	 RAID-0		*Performance(great read and write 	*No data 			100% assuming the drives are 	2
	 		performance)				redundancy			the same size	
			*great capacity(the best of any		*Poor MTTF
			standard RAID cofigurations)
	 --------------------------------------------------------------------------------------------------------------------------------------
	 RAID-1		*Great data				*Worst capacity ulilization	50%assuming two drives of the 	2
	 		redundancy/availability			of single RAID levels		same size
			*Great MTTF				*Good read performance,
								limited write performance
	 --------------------------------------------------------------------------------------------------------------------------------------
	 RAID-4		*Good data redundancy/avalilability	*Single parity dsik(causes 	(n-1)/n where n is the number 	3(have to be 
	 		*Good read performance since all of 	bottleneck)			of drives			identical)
			the drive are read at the same time	*Write performance is not 
			*Can lose one drive without losing	that good because of the 
			data					bottleneck of the parity drive
	 --------------------------------------------------------------------------------------------------------------------------------------
	 RAID-5		*Good data redundancy/avalilability	*Write performance is adequate	(n-1)/n where n is the number	3(have to be
	 		*Very good read performance since all	*Write performance for small	of drives			identical)
			of the drives can be read at the same	I/O is not good at all
			time
			*Write performance is adequate(better
			than RAID-4)
			*Can lose one drive without losing 
			data
	 --------------------------------------------------------------------------------------------------------------------------------------
	 RAID-6		*Excellent data redundancy/avalila-	*Write performance is not that	(n-2)/n where n is the number	4(have to be
	 		-bility					good - worse than RAID-5	of drives			identical)
	 		*Very good read performance since 	*Write performance for small
			all of drives can be read at the 	I/O is not good at all
			same time				*more computational horsepower
			*Can lose twoo drives without losing	is required for parity computations
			data
	 --------------------------------------------------------------------------------------------------------------------------------------
	 RAID-01	*very good performance(both read and 	*very poor storage efficiency	50% assuming the drives are	4(Note: you must
	 		write)					(50%)				the same size			use even number
			*reasonable data redundancy(can tole-	*with nested raid levels,more					of disks)
			-rate the loss of any one disk)		data redundancy is desired 
			*number of disks lost with no loss 	beyond the loss of one disk
			of data access varies from 1 to n/2.	*all disks have to be used in
			however, the n/2 is unlikely to happen	rebuild
	 --------------------------------------------------------------------------------------------------------------------------------------
	 RAID-10	*very good performance(both read and	*very poor storage efficiency	*50% assuming the drivers are	4(Note: you must
	 		write)					*with nested raid levels, more	the same size			use an even number
			*reasonable data redundancy		data redundancy is desired					of disks)
			*only one disk is involved in rebuild	beyond the loss of one disk
	 --------------------------------------------------------------------------------------------------------------------------------------
	 RAID-05	*excellent read performance 		*you have to use at least 6	*storage efficiency = (number	6(Note: you must
	 		*very good write performance		drives 				of RAID-0 groups - 1) / (number	use a non-prime
			*reasonable data redundancy 		*all disks have to be used in	of RAID-0 groups)		number of disks
			*reasonable storage efficiency and	rebuild								素数)
			capacity
	 --------------------------------------------------------------------------------------------------------------------------------------
	 RAID-50	*excellent read performance		*you have to use at least 6	*storage Efficiency = (number	6(Note: you must
	 		*very good write performance		drives				of RAID-5 groups - 1) / (number	use a non-prime
			*reasonable data redundancy						of RAID-5 groups)		number of disks)
			*only have to use the drives in the 
			RAID-5 group of the failed drive
			during rebuild(shorter rebuild times)
	 --------------------------------------------------------------------------------------------------------------------------------------
	 RAID-15	...
	 --------------------------------------------------------------------------------------------------------------------------------------
	 RAID-51	...
	 --------------------------------------------------------------------------------------------------------------------------------------
#日志文件系统
	目前Linux的日志文件系统主要有：在ext2基础上开发的ext3，根据面向对象思想设计的reiserfs，由SGI IRIX系统移植过来的XFS，由IBM
		AIX移植过来的JFS，其中ext3磁盘结构和ext2完全一样，而后三种文件系统广泛使用了B树以提高文件系统的效率
	1. ext3
		1.1 ext3既可以只对元数据做日志，也可以同时对文件数据块做日志。具体来说，ext3提供以下三种日志模式
		* 日志(Journal)
		文件系统所有数据和元数据的改变都可以记入日志。这是最安全和最慢的ext3日志模式
		* 预定(Ordered)
		只有对文件系统元数据的改变才记入日志。然而，ext3文件系统把元数据和相关的数据块进行分组，以便把元数据写入磁盘之前写入数据块。
			这是缺省的ext3日志模式
		* 写回模式(writeback)
		只有对文件系统元数据的改变才记入日志，这是在其他日志文件系统发现的方法，也是最快的模式
		1.2 日志块设备(JBD)
		ext3文件系统本身不处理日志，而是利用日志块设备(Journaling Block Device, JBD)的通用内核层



















































