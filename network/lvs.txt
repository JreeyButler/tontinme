------Heartbeat
------lvs的四种ip负载均衡技术
------lvs的部署
------what is vip
------some lvs tools


#Heartbeat
	Heartbeat is a daemon that provides cluster infrastructure (communication and membership) services to
  its clients. This allows clients to know about the presence (or disappearance!) of peer processes on other
  machines and to easily exchange messages with them.
	In order to be useful to users, the Heartbeat daemon needs to be combined with a cluster resource
  manager (CRM) which has the task of starting and stopping the services (IP addresses, web servers, etc)
  that cluster will make highly available. The canonical cluster resource manager typically associated with
  Heartbeat is Pacemaker, a highly scalable and feature-rich implementation that supports both Heartbeat and,
  alternatively, the Corosync cluster messaging layer.
      Heartbeat include some components:
	  * Communication module
	  * Cluster Consensus Membership
	  * Cluster Plumbing Library
	  * IPC Library
	  * Non-blocking logging daemon
      #For more:
	http://www.linux-ha.org/doc/users-guide/_components.html
	The Heartbeat communication module provides strongly authenticated, locally-ordered multicast messaging
  over basically any media, IP-based or not. Heartbeat supports cluster communications over the following
  network link types:
	  * unicast UDP over IPv4;
	  * broadcast UDP over IPv4;
	  * multicast UDP over IPv4;
	  * serial link communications
	Heartbeat can detect node failure reliably in less than a half-second. It will register with the system
  watchdog timer if configured to do so. The heartbeat layer has an API which provides the following classes of services:
	  * intra-cluster communication - sending and receiving packets to cluster nodes
	  * configuration queries
	  * connectivity information (who can the current node hear packets from) - both for queries and state change notifications
	  * basic group membership services

#lvs的四种ip负载均衡技术

1. VS/NAT
	在一组服务器前有一个调度器，它们是通过switch/hub相连接的。即
	Load Balancer ==> switch/hub ==> Real Server[1..n]
	客户通过vip访问网络服务时，请求报文到达调度器，调度器根据连接调度算法从一组真实服务器中选出一台服务器。
  将报文的目标地址vip改写成选定服务器的地址，报文的目标端口改写成选定服务器的相应端口，最后将修改后的报文发送给
  选出的服务器。同时，调度器在连接hash表中记录这个连接，当这个连接的下一个报文到达时，从连接hash表中可以得到原
  选定服务器的地址和端口，进行同样的改写操作，并将报文传给原选定的服务器。当来自真实服务器的响应报文经过调度器时，
  调度器将报文的源地址和源端口改为vip和相应端口，再把报文转发给用户。我们在连接上引入了一个状态机，不同的报文会
  使得连接处于不同状态，不同状态会有不同的超时值。在tcp连接中，根据标准的tcp有限状态机进行状态迁移。
	NAT（Network Address Translation）是一种外网和内网地址映射的技术。NAT模式下，网络报的进出都要经过LVS的处理。
  LVS需要作为RS的网关。当包到达LVS时，LVS做目标地址转换（DNAT），将目标IP改为RS的IP。RS接收到包以后，仿佛是客户端
  直接发给它的一样。RS处理完，返回响应时，源IP是RS IP，目标IP是客户端的IP。这时RS的包通过网关（LVS）中转，LVS会做
  源地址转换（SNAT），将包的源地址改为VIP，这样，这个包对客户端看起来就仿佛是LVS直接返回给它的。客户端无法感知到
  后端RS的存在。

	请求和响应的数据报文都需要经过负载调度器，当真实服务器的数目在10-20之间时，负载调度器将成文整个集群系统
  的新瓶颈。

2. VS/TUN	ip隧道
	ip隧道主要用于移动主机和虚拟私有网络，在其中隧道都是静态建立的
	它的连接调度和管理与VS/NAT中的一样，只是它的报文转发方法不同。调度器根据各个服务器的负载情况，动态的选择一台
  服务器，将请求报文封装在另一个IP报文中，再将封装后的ip报文转发给选出的服务器；服务器收到报文后，先将报文解封获得原来
  目标地址为vip的报文，服务器发现vip地址被配置在本地的ip隧道设备上，所以就处理这个请求，然后根据路由表将响应报文直接返回
  给客户。
	在这里需要指出的是，根据缺省的tcp/ip协议栈处理，请求报文的目标地址为vip，响应报文的源地址肯定也为vip，所以响应
  报文不需要做任何修改，可以直接返回给客户，客户认为得到正常服务，而不会知道究竟是那一台服务器处理的。

3. VS/DR	直接路由
	跟VS/TUN相同，负载调度器只负责调度请求，而服务器直接将响应返回给客户，可以极大的提高整个集群系统的吞吐量。
	调度器和服务器组都必须在同一个VLAN中，vip地址为调度器和服务器共享，调度器配置的vip地址对外是可见的，用于接收虚拟
  服务的请求报文；所有的服务器把vip地址配置在各自的non-arp网络设备上，它对外面是不可见的，只是用于处理目标地址为vip的网络请求
	DR模式下需要LVS和绑定同一个VIP（RS通过将VIP绑定在loopback实现）
	它的连接调度和管理与vs/nat和vs/tun中的一样，它的报文转发方式又有不同，将报文直接路由给目标服务器。在vs/dr中，调度器
  根据各个服务器的负载情况，动态的选择一台服务器，不修改也不封装ip报文，而是将数据帧的mac地址改为选出的服务器的mac地址，再
  将修改后的数据帧在与服务器组的局域网上发送。因为数据帧的mac地址是选出的服务器，所以服务器肯定可以收到这个数据帧，从中可以获得
  这个该ip报文，当服务器发现报文的目标地址vip是在本地的网络设备上，服务器处理这个报文，然后根据路由表将响应报文直接返回给客户。
	一个请求过来时，LVS只需要将网络帧的MAC地址修改为某一台RS的MAC，该包就会被转发到相应的RS处理，注意此时的源IP和目标IP都没变，
LVS只是做了一下移花接木。RS收到LVS转发来的包，链路层发现MAC是自己的，到上面的网络层，发现IP也是自己的，于是这个包被合法地接受，
RS感知不到前面有LVS的存在。而当RS返回响应时，只要直接向源IP（即用户的IP）返回即可，不再经过LVS。
	和vs/tun一样，请求报文的目标地址为vip，响应报文的源地址肯定也为vip，所以响应报文不需要作任何修改，可以直接返回给客户，
  客户认为得到正常的服务，而不会知道是哪一台服务器处理的
	DR模式是性能最好的一种模式。

	#PS: RS为什么要配置和lvs一样的vip，并且绑定在loopback上
	1、LVS server收到目标地址为VIP的请求包后，将MAC地址改成RS的MAC地址，并通过交换机(链路层)发给RS.
	2、RS的链路层收到请求后，往上传给IP层。IP层需要验证请求的目标IP地址。所以RS需要配置一个VIP的loopback device。
  这样RS的IP层收到报文后，会往上递交给传输层。之所以配置成loopback device，是因为loopback device对外不可见，不会跟LVS server的VIP冲突。
	3、RS处理完成后，将应答包直接返回给客户端。若是公网服务器，则RS需要连上互联网（公网IP或者网关）才能将应答包返回。

4. 三种方法的优缺点
	-		VS/NAT		VS/TUN		VS/DR
	Server		any		Tunneling	Non-arp device
	server network	private		LAN/WAN		LAN
	server number	loc(10~20)	High(100)	High(100)
	server gateway	load balancer	own router	own router

5. FULLNAT lvs
	无论是DR还是NAT模式，不可避免的都有一个问题：LVS和RS必须在同一个VLAN下，否则LVS无法作为RS的网关。
	这引发的两个问题是：
	1、同一个VLAN的限制导致运维不方便，跨VLAN的RS无法接入。
	2、LVS的水平扩展受到制约。当RS水平扩容时，总有一天其上的单点LVS会成为瓶颈。
	Full-NAT由此而生，解决的是LVS和RS跨VLAN的问题，而跨VLAN问题解决后，LVS和RS不再存在VLAN上的从属关系，可以做到多个LVS
  对应多个RS，解决水平扩容的问题。
	在包从LVS转到RS的过程中，源地址从客户端IP被替换成了LVS的内网IP。
	内网IP之间可以通过多个交换机跨VLAN通信。
	当RS处理完接受到的包，返回时，会将这个包返回给LVS的内网IP，这一步也不受限于VLAN。
	LVS收到包后，在NAT模式修改源地址的基础上，再把RS发来的包中的目标地址从LVS内网IP改为客户端的IP。
	Full-NAT主要的思想是把网关和其下机器的通信，改为了普通的网络通信，从而解决了跨VLAN的问题。采用这种方式，LVS和RS
  的部署在VLAN上将不再有任何限制，大大提高了运维部署的便利性。
	Full-NAT替换了client IP为local ip，但是很多时候，后端服务需要client ip，以便进行逻辑处理，因此配置FULL-NAT时，需要
  将lvs server打上内核补丁，以便能够捕获client ip

6. Session
	客户端与服务端的通信，一次请求可能包含多个TCP包，LVS必须保证同一连接的TCP包，必须被转发到同一台RS，否则就乱套了。
  为了确保这一点，LVS内部维护着一个Session的Hash表，通过客户端的某些信息可以找到应该转发到哪一台RS上。

7. LVS集群
	Full-NAT一般搭建成LVS集群使用，单机时也有瓶颈，由于相比NAT和DR，解决了跨VLAN的问题，因此更适合做成LVS集群
	其他模式的LVS也能够扩容做成lvs集群，但是要求是所有lb和rs在同一VLAN，网络局限性较大，路由表难以维护，容易出现VLAN ID冲突。
	LVS集群一般采用三层设备（三层交换机） + 多台LVS server，需要配置OSPF。
	LVS调度机自由伸缩，横向线性扩展（最多机器数受限于三层设备允许的等价路由数目 maximum load-balancing ）；
	LVS机器同时工作，不存在备机，提高利用率；
	做到了真正的高可用，某台LVS机器宕机后，不会影响服务（但因为华3设备ospfd调度算法的问题，一台宕机会使所有的长连接的
  断开重连，目前还无法解决;思科的设备已经支持一至性哈希算法，不会出现这个问题）。

8. 容灾
	容灾分为RS的容灾和LVS的容灾。
	RS的容灾可以通过LVS定期健康检测实现，如果某台RS失去心跳，则认为其已经下线，不会在转发到该RS上。
	LVS的容灾可以通过主备+心跳的方式实现。主LVS失去心跳后，备LVS可以作为热备立即替换。
	容灾主要是靠KeepAlived来做的。


#lvs的部署
	There are several software packages in conjuction with LVS to provide high availability of the whole system, such as
  RedHat Piranha, Keepalived, UltraMonkey, heartbeat plus Idirectord, and heartbeat plus mon. Also, you can install from 
  source code(setup from command line or from a configure script which sets up an LVS with a single director).
	For further:
  #http://www.linuxvirtualserver.org/HighAvailability.html
1. Piranha
	Piranha 方案是基于 LVS 基础上设计的一套负载均衡高可用解决方案. 
	LVS 运行在一对有相似配置的计算机上： 
	  一个作为活动 LVS Router(Active LVS Router)， 
	  一个作为备份 LVS Router(Backup LVS Router)。 
	活动 LVS Router 服务有两个角色： 
	  *  均衡负载到真实服务器上。 
	  *  检查真实服务器提供的服务是否正常。 
	备份LVS Router用来监控活动的LVS Router，以备活动的LVS Router失败时由备份LVS Router 接管。

	>Pulse
	Pulse进程运行在active和backup lvs上，在备份 LVS Router 上，pulse 发送一个心跳（heartbeat）到活动 LVS Router 的公网接口
  上以检查活动 LVS Router 是否正常。 
	在活动 LVS Router 上，pulse 启动 lvs进程并响应来自于备份 LVS Router 的心跳。
	>lvsd
	lvsd进程运行在active lvs上，调用ipvsadm工具去配置和维护ipvs路由表，并为每一个在real server上的虚拟服务启动一个nanny进程。
	>nanny
	每一个nanny进程去检查real server上的虚拟服务状态，并将故障情况通知lvsd进程。假如有故障被发现，lvsd进程通知ipvsadm在ipvs
  路由表中将此节点删除
	>send_arp
	如果backup lvs未收到active lvs的响应，它将调用send_arp将虚拟ip地址再分配到backup lvs的公网接口上。并在公网接口和局域网接口
  分别发送一个命令去关掉active lvs上的lvsd进程，同时启动自己的lvsd进程来调度客户请求。

	/etc/sysconfig/ha/lvs.cf       //由http://ip:3636 web界面配置的配置文件写入此文件. 
    	/etc/init.d/piranha‐gui start   //启动 piranha 服务的WEB配置界面. 
    	/etc/init.d/pulse           //启动 piranha 服务读取的就是/etc/sysconfig/ha/lvs.cf. 

#what is vip
	The vip is the address which you want to load balance. The vip is usually a secondary address so that the vip can be swapped
  between two directors on failover(the vip used to be an alias(eg. eth0:1)).
	The vip is the ip address of service, not the ip address of any of the particular systems used in providing the service
	the vip be moved from one director to another backup director if a fault is detected(typically this is done by using mon and
  heartbeat, or something similar). The director can have multiple vips.
	The realservers have to be configured to work with vips on the director (use loopback or hidden profiles, otherwise,
  this including handling the 'arp problems').
	There can be "persistent problem" issues, if you are using cookies or https, or anything else that expects the realserver 
  fullfilling the requests to have some connection state information.

#some lvs tools
  ipvsman
    ipvsman is a curses based GUI to the ipvs loadbalancer written in python. ipvsman is a monitoring instance of ipvs to achive the
	  desired state of loadbalancing as ldirectord or keepalived do.
  ipvsadm
    ipvsadm is the user code interface to LVS.
  schedulers
    The Scheduler is the part of the ipvs kernel code which decides which realserver will get the next new connection.
